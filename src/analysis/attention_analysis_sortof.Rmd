---
title: "Analysis of 'Sort of' Attention"
author: "Sean Trott and Pam Rivi√®re"
date: "November 20, 2024"
output:
  html_document:
    keep_md: yes
    toc: yes
    toc_float: yes
  # pdf_document: 
  #    fig_caption: yes
  #    keep_md: yes
  #    keep_tex: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dpi = 300, fig.format = "png")
```


```{r include=FALSE}
library(tidyverse)
library(lmtest)
library(forcats)
library(broom)
library(lme4)
library(viridis)
library(ggridges)
library(lmerTest)
library(ggrepel)
library(ggcorrplot)

all_colors <- viridis::viridis(10, option = "mako")
my_colors <- all_colors[c(3, 5, 7)]  # Selecting specific colors from the palette
```

# Load Pythia data


Here, we analyze data looking at the average attention each head gives from the ambiguous token to the disambiguating token.

```{r}
# setwd("/Users/seantrott/Dropbox/UCSD/Research/Ambiguity/SSD/entangled_meanings/src/analysis/")
directory_path <- "../../data/processed/rawc/pythia/attention_sortof_check/"
csv_files <- list.files(path = directory_path, pattern = "*.csv", full.names = TRUE)
csv_list <- csv_files %>%
  map(~ read_csv(.))
df_pythia_models <- bind_rows(csv_list) %>%
  mutate(Layer = Layer + 1,
         Head = Head + 1)

table(df_pythia_models$mpath)

```


# Analysis of 14m

## Attention at final step

```{r}
df_final_step = df_pythia_models %>%
  filter(mpath == "EleutherAI/pythia-14m") %>%
  filter(step == 143000)
nrow(df_final_step)

df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention = mean(Attention))


df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Attention to Disambiguating Word") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Attention to Disambiguating Word") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")



### 
df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention_1back = mean(Attention_1back))


df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention_1back)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Avg. Attention to Previous Token") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Avg. Attention to Previous Token") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")



### 
df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention_inserted_string = mean(Attention_inserted_string))


df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention_inserted_string)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Attention to Inserted String") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Attention to Inserted String") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")


```



## Attention over time


### Attention by head

```{r}
df_attention_over_time = df_pythia_models %>% 
  filter(mpath %in% c("EleutherAI/pythia-14m")) %>%
  group_by(mpath, revision, Layer, Head, step, n_params) %>%
  summarise(mean_attention = mean(Attention)) %>%
  mutate(step_modded = step + 1) %>%
  group_by(mpath, step, Head)

df_attention_over_time %>%
  ggplot(aes(x = step_modded,
             y = mean_attention,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Disambiguating Word",
       color = "Attention Head") +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE) +
  facet_wrap(~Layer)


df_attention_over_time %>%
  filter(Layer == 3) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Disambiguating Word",
       color = "Attention Head") +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE)


### 1back
df_attention_over_time = df_pythia_models %>% 
  filter(mpath %in% c("EleutherAI/pythia-14m")) %>%
  group_by(mpath, revision, Layer, Head, step, n_params) %>%
  summarise(mean_attention_1back = mean(Attention_1back)) %>%
  mutate(step_modded = step + 1) %>%
  group_by(mpath, step, Head)

df_attention_over_time %>%
  # filter(Layer == 3) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention_1back,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Previous Token",
       color = "Attention Head") +
  scale_x_log10() +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE) +
  facet_wrap(~Layer)


### Attention to 'of'
df_attention_over_time = df_pythia_models %>% 
  filter(mpath %in% c("EleutherAI/pythia-14m")) %>%
  group_by(mpath, revision, Layer, Head, step, n_params) %>%
  summarise(mean_attention_inserted_string = mean(Attention_inserted_string)) %>%
  mutate(step_modded = step + 1) %>%
  group_by(mpath, step, Head)

df_attention_over_time %>%
  # filter(Layer == 3) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention_inserted_string,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Inserted String",
       color = "Attention Head") +
  scale_x_log10() +
  # scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE) +
  facet_wrap(~Layer)


df_attention_over_time %>%
  filter(Layer == 3) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention_inserted_string,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Inserted String",
       color = "Attention Head") +
  scale_x_log10() +
  # scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE)


```



## Subtraction analysis

Finally, we compare attention to 'of' to attention to previous tokens more generally.

```{r}
t_test_results <- df_pythia_models %>%
  group_by(mpath, Layer, Head, n_params, step) %>%
  summarise(
    t_test = list(
      t.test(
        Attention_inserted_string,
        Attention_1back,
        paired = TRUE,
        alternative = "greater"  # One-tailed test for greater values
      )
    ),
    .groups = "drop"
  ) %>%
  mutate(
    p_value = map_dbl(t_test, ~ .x$p.value),          # Extract p-value
    t_statistic = map_dbl(t_test, ~ .x$statistic)    # Extract t-statistic
  )


# Apply multiple testing correction
t_test_results <- t_test_results %>%
  group_by(mpath) %>%
  mutate(
    p_adjusted = p.adjust(p_value, method = "fdr")  # Adjust p-values (FDR)
  ) %>%
  mutate(p_adj2 = case_when(
    p_adjusted < .05 ~ p_adjusted,
    p_adjusted >= .05 ~ 1
  ))

# Filter significant results
significant_results <- t_test_results %>%
  filter(p_adjusted < 0.05) 


significant_results %>%
  filter(step == 51000) %>%
  ### Redo with step == 143000
  ggplot(aes(x = Layer, y = Head, fill = -log10(p_adjusted))) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "red", name = "-log10(FDR p-value)") +
  theme_minimal() +
  labs(
    title = "Significant Heads/Layers",
    x = "Layer",
    y = "Head"
  ) 

### Comparison for l3, h2 by step
df_pythia_models %>%
  filter(mpath == "EleutherAI/pythia-14m") %>%
  filter(step %in% c(1, 1000, 2000, 143000)) %>%
  filter(Head == 2) %>%
  filter(Layer == 3) %>%
  select(step, Attention_1back, Attention_inserted_string) %>%
  pivot_longer(cols = c(Attention_1back, Attention_inserted_string),
               names_to = "Focus", values_to = "Attention") %>%
  ggplot(aes(x = Attention, 
             y = factor(step),
             fill = Focus)) +
  geom_density_ridges2(aes(height = ..density..), 
                       color = gray(0.25), 
                       alpha = .7, 
                       scale = .85, 
                       size = 0,
                       stat = "density") +
  theme_minimal() +
  labs(x = "Attention (L3, H2)",
       y = "Step",
       fill = "Focus") +
  scale_fill_viridis_d(option = "mako", 
                       labels = c("1-back", "Inserted String")) +
  theme(text = element_text(size = 15),
        legend.position = "bottom")

```


# Analysis of 410m


## Attention at final step

```{r}
df_final_step = df_pythia_models %>%
  filter(mpath == "EleutherAI/pythia-410m") %>%
  filter(step == 143000)
nrow(df_final_step)

df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention = mean(Attention)) %>%
  mutate(attn_disamb_sort_of = mean_attention)


write.csv(df_attn_by_head, "../../data/processed/rawc/pythia/summaries/sortof_410m.csv")

df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Attention to Disambiguating Word") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Attention to Disambiguating Word") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")



### 
df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention_1back = mean(Attention_1back))


df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention_1back)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Avg. Attention to Previous Token") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Avg. Attention to Previous Token") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")



### 
df_attn_by_head = df_final_step %>%
  group_by(mpath, Layer, Head, n_params) %>%
  summarise(mean_attention_inserted_string = mean(Attention_inserted_string))


df_attn_by_head %>%
  ggplot(aes(x = Layer,
             y = Head,
             fill = mean_attention_inserted_string)) +
  geom_tile() +
  labs(x = "Layer",
       y = "Head",
       fill = "Attention to Inserted String") +
  scale_fill_gradient2(low = "blue",
                       mid = "white",
                       high = "red",
                       midpoint = 0, 
                       name = "Attention to Inserted String") +
  theme_minimal() +
  theme(text = element_text(size = 15),
        strip.text.y = element_text(angle = 0), # Keep facet labels horizontal
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")


```



## Attention over time


### Attention by head

```{r}
df_attention_over_time = df_pythia_models %>% 
  filter(mpath %in% c("EleutherAI/pythia-410m")) %>%
  group_by(mpath, revision, Layer, Head, step, n_params) %>%
  summarise(mean_attention = mean(Attention)) %>%
  mutate(step_modded = step + 1) %>%
  group_by(mpath, step, Head)

df_attention_over_time %>%
  filter(Layer %in% c(1, 2, 4, 6, 22, 23)) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Disambiguating Word",
       color = "Attention Head") +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE) +
  facet_wrap(~Layer)


df_attention_over_time %>%
  filter(Layer == 3) %>%
  ggplot(aes(x = step_modded,
             y = mean_attention,
             color = factor(Head))) +
  geom_point(size = 3, alpha = .7) +
  geom_line(size = 2, alpha = .7) +
  theme_minimal() +
  labs(x = "Training Step (Log10)",
       y = "Attention to Disambiguating Word",
       color = "Attention Head") +
  scale_x_log10() +
  scale_y_continuous(limits = c(0, .8)) +
    geom_vline(xintercept = 1000, 
             linetype = "dotted", 
             size = 1.2) +
  theme(text = element_text(size = 15),
        legend.position = "bottom") +
  scale_color_viridis(option = "mako", discrete=TRUE)


```

